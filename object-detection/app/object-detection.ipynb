{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "\"As I explained before: you have to do detection with py-faster-rcnn and then a tracking using deep sort.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use mask-r-cnn to get detections in ROI format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[256 256]\n",
      " [128 128]\n",
      " [ 64  64]\n",
      " [ 32  32]\n",
      " [ 16  16]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           coco\n",
      "NUM_CLASSES                    1\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimension 1 in both shapes must be equal, but are 4 and 324. Shapes are [1024,4] and [1024,324]. for 'Assign_682' (op: 'Assign') with input shapes: [1024,4], [1024,324].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 1 in both shapes must be equal, but are 4 and 324. Shapes are [1024,4] and [1024,324]. for 'Assign_682' (op: 'Assign') with input shapes: [1024,4], [1024,324].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-41d231dd032d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Load weights trained on MS-COCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOCO_MODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# COCO Class names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/player-speed/object-detection/app/model.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, exclude)\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2032\u001b[0;31m             \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2034\u001b[0m             \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group_by_name\u001b[0;34m(f, layers, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   3273\u001b[0m                                             weight_values[i]))\n\u001b[1;32m   3274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3275\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2363\u001b[0m                 assign_placeholder = tf.placeholder(tf_dtype,\n\u001b[1;32m   2364\u001b[0m                                                     shape=value.shape)\n\u001b[0;32m-> 2365\u001b[0;31m                 \u001b[0massign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2366\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2367\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking)\u001b[0m\n\u001b[1;32m    592\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0massignment\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \"\"\"\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m    274\u001b[0m     return gen_state_ops.assign(\n\u001b[1;32m    275\u001b[0m         \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    277\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m     58\u001b[0m         \u001b[0;34m\"Assign\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3160\u001b[0m         op_def=op_def)\n\u001b[1;32m   3161\u001b[0m     self._create_op_helper(ret, compute_shapes=compute_shapes,\n\u001b[0;32m-> 3162\u001b[0;31m                            compute_device=compute_device)\n\u001b[0m\u001b[1;32m   3163\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[0;34m(self, op, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3206\u001b[0m     \u001b[0;31m# compute_shapes argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3208\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3209\u001b[0m     \u001b[0;31m# TODO(b/XXXX): move to Operation.__init__ once _USE_C_API flag is removed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2425\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs_c_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2426\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2427\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_set_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2398\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2400\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2330\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 1 in both shapes must be equal, but are 4 and 324. Shapes are [1024,4] and [1024,324]. for 'Assign_682' (op: 'Assign') with input shapes: [1024,4], [1024,324]."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import coco\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
    "\n",
    "########################################\n",
    "\n",
    "\n",
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()\n",
    "\n",
    "\n",
    "\"\"\"Create Model and Load Trained Weights\"\"\"\n",
    "\n",
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)\n",
    "\n",
    "# COCO Class names\n",
    "# Index of the class in the list is its ID. For example, to get ID of\n",
    "# the teddy bear class, use: class_names.index('teddy bear')\n",
    "\n",
    "# class_names = ['person']\n",
    "\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "               'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "\n",
    "# Load a random image from the images folder\n",
    "# file_names = next(os.walk(IMAGE_DIR))[2]\n",
    "# image = skimage.io.imread('os.path.join(IMAGE_DIR, random.choice(file_names))')\n",
    "image = skimage.io.imread('images/1045023827_4ec3e8ba5c_z.jpg')\n",
    "\n",
    "# Run detection\n",
    "results = model.detect([image], verbose=1)\n",
    "\n",
    "# Visualize results\n",
    "r = results[0]\n",
    "print(r['rois'].shape)\n",
    "print(r['scores'].shape)\n",
    "print(r['scores'])\n",
    "\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import skvideo.io\n",
    "import numpy as np\n",
    "import coco\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "\n",
    "# Mask-R-CNN\n",
    "MASKRCNN_DIR = 'maskrcnn'\n",
    "MODEL_DIR = os.path.join(\"logs\")\n",
    "COCO_MODEL_PATH = os.path.join(\"mask_rcnn_coco.h5\")\n",
    "\n",
    "VIDEO_DIR = 'videos'\n",
    "VIDEO_FILE = 'transition.mp4'\n",
    "video = os.path.join(VIDEO_DIR, VIDEO_FILE)\n",
    "\n",
    "# Save video frames\n",
    "FRAMES_DIR = 'frames'\n",
    "\n",
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "\n",
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)\n",
    "\n",
    "def get_detections_frame(model, image, frame_idx):\n",
    "    results = model.detect([image], verbose=0)\n",
    "    rois = results[0]['rois']\n",
    "    confs = results[0]['scores']\n",
    "    \n",
    "    detections = np.zeros([len(rois), 10])\n",
    "    \n",
    "    for idx, coord in enumerate(rois):\n",
    "        conf = confs[idx]\n",
    "        detections[idx] = to_mot_format(frame_idx, coord, conf)\n",
    "    \n",
    "    return detections\n",
    "\n",
    "\n",
    "def write_to_csv(csv_file, row_titles = None, new_row = None, new_file = 'no'):\n",
    "    if new_file == 'yes':\n",
    "        rwa = 'w'\n",
    "    else:\n",
    "        rwa = 'a'\n",
    "    with open(csv_file, rwa) as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',',quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        if row_titles:\n",
    "            writer.writerow(row_titles)\n",
    "        if new_row:\n",
    "            writer.writerow(new_row)\n",
    "    csvfile.close()\n",
    "\n",
    "def to_mot_format(frame_idx, coord, conf):\n",
    "    \"\"\"\n",
    "    Input coordinates: \n",
    "    (y1, x1, y2, x2)\n",
    "    \n",
    "    Output coordinates: \n",
    "    (frame, id, bb_left, bb_top, bb_width, bb_height, -1, -1, -1, -1)\n",
    "    \"\"\"\n",
    "    filler = -1\n",
    "    bb_left = coord[1]\n",
    "    bb_top = coord[0]\n",
    "    bb_width = coord[3] - coord[1]\n",
    "    bb_height = coord[2] - coord[0]\n",
    "\n",
    "    # Rearrange coordinates\n",
    "    coord = np.array([frame_idx + 1, \n",
    "                      filler,\n",
    "                      bb_left,\n",
    "                      bb_top,\n",
    "                      bb_width,\n",
    "                      bb_height,\n",
    "                      conf,\n",
    "                      filler,\n",
    "                      filler,\n",
    "                      filler])\n",
    "    \n",
    "    return coord\n",
    "\n",
    "def get_detections_video(video):\n",
    "    \"\"\"\n",
    "    Get ROI detections from video using Mask-R-CNN and save in \n",
    "    MOTChallenge format\n",
    "    \"\"\"    \n",
    "    videodata = skvideo.io.vread(video)\n",
    "    num_frames = len(videodata)\n",
    "    det_file = open('detections.txt', 'ab')\n",
    "    \n",
    "    for idx, frame in enumerate(videodata):\n",
    "        try:\n",
    "            print(\"PROCESSING IMAGE {} / {}\".format(idx, num_frames))\n",
    "            detection = get_detections_frame(model, frame, idx)\n",
    "            np.savetxt(det_file, detection, delimiter=',', fmt='%1.2f')\n",
    "            print(\"DONE\")\n",
    "        except:\n",
    "            print(\"FRAME {} NOT PROCESSED\".format(idx))\n",
    "\n",
    "        det_file.flush()\n",
    "    \n",
    "    det_file.close()\n",
    "    \n",
    "    print(\"FINISHED!\")\n",
    "\n",
    "get_detections_video(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate detection features using Deep SORT and append to detections file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import generate_detections\n",
    "import deep_sort.detection\n",
    "import numpy as np\n",
    "import skvideo.io\n",
    "\n",
    "DETECTION_DIR = 'detections'\n",
    "\n",
    "VIDEO_DIR = 'videos'\n",
    "VIDEO_FILE = 'transition.mp4'\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join('resources', 'networks', 'mars-small128.pb')\n",
    "\n",
    "encoder = generate_detections.create_box_encoder(CHECKPOINT_PATH)\n",
    "video = os.path.join(VIDEO_DIR, VIDEO_FILE)\n",
    "name_out = 'generated_detections'\n",
    "detection_file = 'detections.txt'\n",
    "\n",
    "def generate_detections_features(encoder, video, name_out, detection_file):\n",
    "\n",
    "    \"\"\"Generate detections with features. Modification with video\"\"\"\n",
    "    detections_in = np.loadtxt(detection_file, delimiter=',')\n",
    "    detections_out = []\n",
    "\n",
    "    frame_indices = detections_in[:, 0].astype(np.int)\n",
    "    min_frame_idx = frame_indices.astype(np.int).min()\n",
    "    max_frame_idx = frame_indices.astype(np.int).max()\n",
    "\n",
    "    videodata = skvideo.io.vread(video)\n",
    "    \n",
    "\n",
    "    for frame_idx in range(min_frame_idx, max_frame_idx + 1):\n",
    "        print(\"Frame %05d/%05d\" % (frame_idx, max_frame_idx))\n",
    "        mask = frame_indices == frame_idx\n",
    "        rows = detections_in[mask]\n",
    "\n",
    "        if frame_idx not in frame_indices:\n",
    "            print(\"WARNING could not find image for frame %d\" % frame_idx)\n",
    "            continue\n",
    "            \n",
    "#         camera.set(cv2.CAP_PROP_POS_FRAMES, frame_idx-1);\n",
    "#         (grabbed, bgr_image) = camera.read()\n",
    "#         bgr_image = cv2.imread(image_filenames[frame_idx], cv2.IMREAD_COLOR)  \n",
    "\n",
    "        bgr_image = videodata[frame_idx]\n",
    "\n",
    "        features = encoder(bgr_image, rows[:, 2:6].copy())\n",
    "        detections_out += [np.r_[(row, feature)] for row, feature\n",
    "                           in zip(rows, features)]\n",
    "\n",
    "    # output_filename = os.path.join(output_dir, \"%s.npy\" % sequence)\n",
    "    np.save(name_out, np.asarray(detections_out), allow_pickle=False)\n",
    "    \n",
    "generate_detections_features(encoder, video, name_out, detection_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Deep SORT tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from deep_sort import nn_matching\n",
    "from deep_sort.deep_sort_app import create_detections\n",
    "from deep_sort.application_util import preprocessing\n",
    "from deep_sort.tracker import Tracker\n",
    "\n",
    "\n",
    "def tracking(detections_file):\n",
    "    \"\"\" Track objects\"\"\"\n",
    "    detections_file = np.load(detections_file)\n",
    "    min_frame_idx = int(detections_file[:, 0].min())\n",
    "    max_frame_idx = int(detections_file[:, 0].max())\n",
    "    min_confidence = 0.3\n",
    "    min_detection_height = 0\n",
    "    nms_max_overlap = 0.3\n",
    "    \n",
    "    videodata = skvideo.io.vread(video)\n",
    "    \n",
    "    if (display):\n",
    "        plt.ion()\n",
    "        fig = plt.figure()\n",
    "\n",
    "    for frame_idx in range(min_frame_idx, max_frame_idx + 1):\n",
    "        \n",
    "        frame = videodata[frame_idx]\n",
    "        \n",
    "        print(\"Processing frame {}\".format(frame_idx + 1))\n",
    "\n",
    "        # Load image and generate detections.\n",
    "        detections = create_detections(detections_file, frame_idx, min_detection_height)\n",
    "        detections = [d for d in detections if d.confidence >= min_confidence]\n",
    "\n",
    "        # Run non-maxima suppression.\n",
    "        boxes = np.array([d.tlwh for d in detections])\n",
    "        scores = np.array([d.confidence for d in detections])\n",
    "        indices = preprocessing.non_max_suppression(boxes, nms_max_overlap, scores)\n",
    "        detections = [detections[i] for i in indices]\n",
    "\n",
    "        # Update tracker.\n",
    "        max_cosine_distance = 0.05\n",
    "        nn_budget = 1\n",
    "        \n",
    "        metric = nn_matching.NearestNeighborDistanceMetric(\n",
    "            \"cosine\", max_cosine_distance, nn_budget)\n",
    "        \n",
    "        tracker = Tracker(metric)\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "\n",
    "        # Update visualization.\n",
    "        if display:\n",
    "            ax1 = fig.add_subplot(111, aspect='equal')\n",
    "            # fn = 'mot_benchmark/%s/%s/img1/%06d.jpg'%(phase,seq,frame)\n",
    "            ax1.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(' Tracked Targets')\n",
    "\n",
    "        # Store results.\n",
    "        for track in tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            bbox = track.to_tlwh()\n",
    "            results.append([\n",
    "                frame_idx+1, track.track_id, bbox[0], bbox[1], bbox[2], bbox[3]])\n",
    "\n",
    "         \n",
    "            if (display):\n",
    "                ax1.add_patch(patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], fill=False, lw=3,ec=colours[track.track_id % 32, :]))\n",
    "                ax1.set_adjustable('box-forced')\n",
    "                plt.text(bbox[0], bbox[1], str(track.track_id))\n",
    "\n",
    "        if(display):\n",
    "            fig.canvas.flush_events()\n",
    "            plt.draw()\n",
    "            ax1.cla()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking('generated_detections.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00 -1.00000000e+00  5.35000000e+02  1.48000000e+02\n",
      "   4.60000000e+01  8.70000000e+01 -1.00000000e+00 -1.00000000e+00\n",
      "  -1.00000000e+00 -1.00000000e+00  6.87755123e-02  5.70516512e-02\n",
      "  -3.92585294e-03 -9.21779424e-02  7.79442713e-02  2.81216837e-02\n",
      "  -1.10583819e-01 -8.56326222e-02 -1.28503721e-02 -6.98725358e-02\n",
      "  -6.12861365e-02 -1.27887800e-01  9.91246477e-03 -5.79381622e-02\n",
      "   1.49324173e-02  3.30629870e-02 -6.68136925e-02  7.01870397e-02\n",
      "  -2.13439222e-02  8.82290825e-02  2.19001293e-01  1.97677523e-01\n",
      "   3.15135322e-03  3.09841838e-02  1.04944520e-01 -6.00195937e-02\n",
      "  -2.49438472e-02  1.26875460e-01 -4.59383279e-02 -9.58584175e-02\n",
      "   8.22890624e-02 -2.12550499e-02 -6.88390210e-02 -3.21026482e-02\n",
      "   1.69488013e-01  4.72694598e-02  5.60176745e-02 -9.21147987e-02\n",
      "   2.78272867e-01 -7.85716847e-02 -4.80469204e-02 -4.89752255e-02\n",
      "  -7.50424117e-02  1.77075639e-01  1.62325446e-02  3.47578302e-02\n",
      "   3.63305137e-02 -1.00752436e-01  7.15530440e-02 -3.28774899e-02\n",
      "  -1.23983294e-01 -9.63387191e-02  9.23549831e-02 -7.84562901e-02\n",
      "   1.27671078e-01  1.99474454e-01 -1.71032310e-01 -1.78648550e-02\n",
      "   9.68833192e-05 -2.15126146e-02 -6.81125745e-02  1.35107309e-01\n",
      "  -2.85344366e-02  1.12145822e-02 -9.90284979e-02 -7.71713182e-02\n",
      "  -4.02548388e-02 -7.34796841e-03 -2.47939453e-02  5.37215583e-02\n",
      "  -5.10503054e-02 -3.96419205e-02 -2.52396218e-03 -1.00281127e-01\n",
      "   1.73258752e-01 -4.48866859e-02 -9.92202386e-02  6.47267997e-02\n",
      "  -7.68359601e-02 -1.96894188e-03  1.52081009e-02 -1.07006028e-01\n",
      "  -8.29830021e-02  1.53061524e-01 -7.21557513e-02 -1.53932169e-01\n",
      "  -7.99779668e-02  3.77908535e-02  2.59414725e-02 -1.71452053e-02\n",
      "   5.01015745e-02  2.22859513e-02  3.04809302e-01 -6.67108074e-02\n",
      "   1.50045648e-01 -1.12849623e-01 -4.86839414e-02 -5.07136062e-02\n",
      "  -7.23406598e-02 -6.32016808e-02 -1.88935921e-02 -4.67298068e-02\n",
      "  -4.40080091e-02  5.66180656e-03 -3.43507677e-02 -7.36239851e-02\n",
      "  -6.78135604e-02 -9.45349261e-02 -5.51689044e-02 -2.87239607e-02\n",
      "  -8.60838518e-02 -3.07527720e-04  3.70823219e-02 -1.67277530e-02\n",
      "  -8.46565738e-02 -7.49371201e-02 -8.12182128e-02 -4.21141945e-02\n",
      "  -1.02742501e-01 -9.63868573e-02 -8.58077705e-02  1.26965130e-02\n",
      "   8.75485316e-02 -1.03183119e-02  2.03588512e-02 -1.09996401e-01\n",
      "  -4.86568399e-02 -6.46045431e-02]\n",
      " [ 0.00000000e+00 -1.00000000e+00  4.05000000e+02  1.50000000e+02\n",
      "   2.60000000e+01  7.60000000e+01 -1.00000000e+00 -1.00000000e+00\n",
      "  -1.00000000e+00 -1.00000000e+00  4.14426327e-02  9.00070295e-02\n",
      "   6.26596659e-02 -1.09701328e-01  1.46599159e-01  2.75928229e-02\n",
      "  -1.04587540e-01 -1.31030276e-01  3.53794731e-02 -9.55963880e-02\n",
      "  -7.67588392e-02 -1.60204008e-01  8.19303095e-03 -2.29526535e-02\n",
      "  -1.35746464e-01  2.13517230e-02 -8.25435966e-02  2.71392930e-02\n",
      "   6.07562810e-02  3.89697179e-02  1.95748538e-01  1.27940863e-01\n",
      "  -3.22099514e-02 -1.25672540e-03  1.77928470e-02  5.37806675e-02\n",
      "  -4.45228219e-02 -6.68687075e-02 -3.45672108e-02 -9.47742909e-02\n",
      "   1.69227079e-01 -8.85189399e-02 -7.17813000e-02  4.39498611e-02\n",
      "  -2.13940721e-03  1.49360299e-01  4.71545123e-02 -8.80960226e-02\n",
      "   3.11673552e-01 -1.31192103e-01 -2.71155629e-02 -6.84113726e-02\n",
      "  -1.74251497e-01  1.57225072e-01 -6.41536117e-02 -1.25746325e-01\n",
      "  -6.27419055e-02 -7.42059425e-02  9.00849625e-02 -3.58463377e-02\n",
      "  -9.34709013e-02 -1.14946790e-01  8.08981434e-02 -6.14925325e-02\n",
      "   8.41021687e-02  1.79596081e-01 -6.82773143e-02  8.98213238e-02\n",
      "  -4.07394916e-02  3.82026434e-02 -4.18493859e-02  6.27679080e-02\n",
      "  -2.56396458e-02  8.82052034e-02 -6.19014278e-02 -1.22354329e-01\n",
      "   7.48643652e-02 -9.00404230e-02 -8.06477070e-02  1.67301316e-02\n",
      "  -7.43936077e-02 -8.99388804e-04 -1.16407633e-01 -2.35145185e-02\n",
      "  -4.18413579e-02 -7.28103220e-02 -1.18038863e-01  6.37851655e-02\n",
      "  -1.14904046e-01 -2.89108083e-02 -1.96559988e-02 -2.24493141e-03\n",
      "  -1.25623316e-01  9.05776024e-02 -8.40065554e-02 -4.41351086e-02\n",
      "  -9.44222435e-02  7.43874758e-02  5.01095094e-02 -4.59359959e-02\n",
      "  -2.21691802e-02 -1.78661961e-02  1.32925704e-01 -1.13525048e-01\n",
      "   1.04075044e-01 -2.26937849e-02 -4.21858951e-02 -4.04647738e-02\n",
      "   2.59259231e-02 -4.26765084e-02 -3.72918546e-02 -8.79556611e-02\n",
      "  -1.05136901e-01 -4.96128686e-02  5.31928651e-02 -1.07924744e-01\n",
      "  -4.16368768e-02 -1.17247358e-01 -1.01785369e-01  3.73845920e-02\n",
      "  -6.65762499e-02  8.57630819e-02  7.36939162e-02 -3.98786254e-02\n",
      "  -5.57293706e-02 -6.64255768e-02 -1.00368902e-01 -8.81474987e-02\n",
      "  -1.03952296e-01 -1.60297267e-02 -1.00902416e-01 -6.68074042e-02\n",
      "   9.63169187e-02 -8.72592479e-02  3.21009047e-02 -1.15129791e-01\n",
      "  -6.93087131e-02 -1.17175870e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.load('generated_detections.npy')\n",
    "\n",
    "print(arr[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
