{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "\"As I explained before: you have to do detection with py-faster-rcnn and then a tracking using deep sort.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use mask-r-cnn to get detections in ROI format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import coco\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"nba-images\")\n",
    "\n",
    "########################################\n",
    "\n",
    "\n",
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()\n",
    "\n",
    "\n",
    "\"\"\"Create Model and Load Trained Weights\"\"\"\n",
    "\n",
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)\n",
    "\n",
    "# COCO Class names\n",
    "# Index of the class in the list is its ID. For example, to get ID of\n",
    "# the teddy bear class, use: class_names.index('teddy bear')\n",
    "\n",
    "# class_names = ['person']\n",
    "\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "               'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "\n",
    "# Load a random image from the images folder\n",
    "file_names = next(os.walk(IMAGE_DIR))[2]\n",
    "image = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))\n",
    "\n",
    "# Run detection\n",
    "results = model.detect([image], verbose=1)\n",
    "\n",
    "# Visualize results\n",
    "r = results[0]\n",
    "print(r['rois'].shape)\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import generate_detections\n",
    "from deep_sort.detection import Detection\n",
    "\n",
    "DEEP_SORT_DIR = 'deep_sort'\n",
    "MASKRCNN_DIR = 'maskrcnn'\n",
    "TEST_IMG = 'maskrcnn/nba-images/frame_000009.jpg'\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(DEEP_SORT_DIR, \"checkpoint\", \"mars-small128.ckpt-68577\")\n",
    "\n",
    "def main():\n",
    "    # Start up your application, then generate the encoder function.\n",
    "    encoder = generate_detections.create_box_encoder(CHECKPOINT_PATH)\n",
    "\n",
    "    # Now, enter the application main loop. Should be something like this:\n",
    "    for image, detections in my_data_source:\n",
    "        # - image is a BGR color image\n",
    "        # - detections is an Nx4 matrix of bounding boxes in\n",
    "        #   format (x, y, w, h) where (x, y) is the top-left corner\n",
    "        #   and (w, h) is the extent\n",
    "        # - features is an Nx128 matrix of corresponding appearance descriptors\n",
    "        features = encoder(image, detections)\n",
    "        \n",
    "        # Generate a list of detections (we simply set the confidence\n",
    "        # score to 1.0 here).\n",
    "        detections = [\n",
    "            Detection(bbox, 1.0, feature) for bbox, feature in\n",
    "            zip(detections, features)]\n",
    "\n",
    "        # Call the tracker\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "\n",
    "        # Go on and visualize the results, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import generate_detections\n",
    "from deep_sort.detection import Detection\n",
    "\n",
    "DEEP_SORT_DIR = 'deep_sort'\n",
    "MASKRCNN_DIR = 'maskrcnn'\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(DEEP_SORT_DIR, \"checkpoint\", \"mars-small128.ckpt-68577\")\n",
    "\n",
    "def main():\n",
    "    # Start up your application, then generate the encoder function.\n",
    "    encoder = generate_detections.create_box_encoder(CHECKPOINT_PATH)\n",
    "\n",
    "    # Now, enter the application main loop. Should be something like this:\n",
    "    for image, detections in my_data_source:\n",
    "        # - image is a BGR color image\n",
    "        # - detections is an Nx4 matrix of bounding boxes in\n",
    "        #   format (x, y, w, h) where (x, y) is the top-left corner\n",
    "        #   and (w, h) is the extent\n",
    "        # - features is an Nx128 matrix of corresponding appearance descriptors\n",
    "        features = encoder(image, detections)\n",
    "        \n",
    "        # Generate a list of detections (we simply set the confidence\n",
    "        # score to 1.0 here).\n",
    "        detections = [\n",
    "            Detection(bbox, 1.0, feature) for bbox, feature in\n",
    "            zip(detections, features)]\n",
    "\n",
    "        # Call the tracker\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "\n",
    "        # Go on and visualize the results, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import skvideo.io\n",
    "import numpy as np\n",
    "import coco\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "\n",
    "# Mask-R-CNN\n",
    "MASKRCNN_DIR = 'maskrcnn'\n",
    "MODEL_DIR = os.path.join(\"logs\")\n",
    "COCO_MODEL_PATH = os.path.join(\"mask_rcnn_coco.h5\")\n",
    "\n",
    "VIDEO_DIR = 'videos'\n",
    "VIDEO_FILE = 'transition.mp4'\n",
    "video = os.path.join(VIDEO_DIR, VIDEO_FILE)\n",
    "\n",
    "# Save video frames\n",
    "FRAMES_DIR = 'frames'\n",
    "\n",
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "\n",
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)\n",
    "\n",
    "def get_detections_frame(model, image, frame_idx):\n",
    "    results = model.detect([image], verbose=0)\n",
    "    rois = results[0]['rois']\n",
    "    \n",
    "    detections = np.zeros([len(rois), 10])\n",
    "    \n",
    "    for idx, coord in enumerate(rois):\n",
    "        detections[idx] = to_mot_format(frame_idx, coord)\n",
    "    \n",
    "    return detections\n",
    "\n",
    "\n",
    "def write_to_csv(csv_file, row_titles = None, new_row = None, new_file = 'no'):\n",
    "    if new_file == 'yes':\n",
    "        rwa = 'w'\n",
    "    else:\n",
    "        rwa = 'a'\n",
    "    with open(csv_file, rwa) as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',',quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        if row_titles:\n",
    "            writer.writerow(row_titles)\n",
    "        if new_row:\n",
    "            writer.writerow(new_row)\n",
    "    csvfile.close()\n",
    "    \n",
    "def to_mot_format(frame_idx, coord):\n",
    "    \"\"\"\n",
    "    Input coordinates: \n",
    "    (y1, x1, y2, x2)\n",
    "    \n",
    "    Output coordinates: \n",
    "    (frame, id, bb_left, bb_top, bb_width, bb_height, -1, -1, -1, -1)\n",
    "    \"\"\"\n",
    "    padding = np.array([-1, -1, -1, -1])\n",
    "    \n",
    "    coord = np.insert(coord, 0, frame_idx)\n",
    "    coord = np.insert(coord, 1, -1)\n",
    "    coord = np.append(coord, padding)\n",
    "    \n",
    "    width = coord[5] - coord[3]\n",
    "    height = coord[4] - coord[2]\n",
    "    \n",
    "    coord[4] = width\n",
    "    coord[5] = height\n",
    "\n",
    "    # Rearrange coordinates\n",
    "    coord = coord[[0, 1, 3, 2, 4, 5, 6, 7, 8, 9]]\n",
    "    \n",
    "    return coord\n",
    "\n",
    "def get_detections_video(video):\n",
    "    \"\"\"\n",
    "    Get ROI detections from video using Mask-R-CNN and save in \n",
    "    MOTChallenge format\n",
    "    \"\"\"    \n",
    "    videodata = skvideo.io.vread(video)\n",
    "    num_frames = len(videodata)\n",
    "    det_file = open('detections.txt', 'ab')\n",
    "    \n",
    "    for idx, frame in enumerate(videodata):\n",
    "        try:\n",
    "            print(\"PROCESSING IMAGE {} / {}\".format(idx, num_frames))\n",
    "            detection = get_detections_frame(model, frame, idx)\n",
    "            np.savetxt(det_file, detection, delimiter=',', fmt='%1.2f')\n",
    "            print(\"DONE\")\n",
    "        except:\n",
    "            print(\"FRAME {} NOT PROCESSED\".format(idx))\n",
    "\n",
    "        det_file.flush()\n",
    "    \n",
    "    det_file.close()\n",
    "    \n",
    "    print(\"FINISHED!\")\n",
    "\n",
    "# def get_detections_video(video):\n",
    "#     \"\"\"\n",
    "#     Get ROI detections from video using Mask-R-CNN and save in \n",
    "#     MOTChallenge format\n",
    "#     \"\"\"    \n",
    "#     camera = cv2.VideoCapture(video)\n",
    "#     num_frames = int(camera.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     count = 0\n",
    "#     det_file = open('detections.txt', 'ab')\n",
    "    \n",
    "#     while(camera.isOpened()):\n",
    "#         success,image = camera.read()\n",
    "#         if success and count > 360:\n",
    "#             print(\"PROCESSING IMAGE {} / {}\".format(count, num_frames))\n",
    "#             detection = get_detections_frame(model, image, count)\n",
    "#             np.savetxt(det_file, detection, delimiter=',', fmt='%1.2f')\n",
    "#             print(\"DONE\")\n",
    "\n",
    "#         else:\n",
    "#             print(\"FRAME {} NOT PROCESSED\".format(count))\n",
    "\n",
    "# #         if count == 0:\n",
    "# #             detections = detection\n",
    "# #         else:\n",
    "# #             detections = np.concatenate((detections, detection))\n",
    "            \n",
    "#         count += 1\n",
    "#         det_file.flush()\n",
    "        \n",
    "#         if (count > (num_frames)):\n",
    "#             camera.release()\n",
    "      \n",
    "#     det_file.close()\n",
    "    \n",
    "#     print(\"FINISHED!\")\n",
    "\n",
    "\n",
    "    \n",
    "get_detections_video(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"The name 'net/images:0' refers to a Tensor which does not exist. The operation, 'net/images', does not exist in the graph.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ec39c868de03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mCHECKPOINT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEEP_SORT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoint'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mars-small128.ckpt-68577'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_detections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_box_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVIDEO_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVIDEO_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mname_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'generated_detections.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/player-speed/object-detection/app/deep_sort/deep_sort/generate_detections.py\u001b[0m in \u001b[0;36mcreate_box_encoder\u001b[0;34m(model_filename, input_name, output_name, batch_size)\u001b[0m\n\u001b[1;32m     98\u001b[0m def create_box_encoder(model_filename, input_name=\"images\",\n\u001b[1;32m     99\u001b[0m                        output_name=\"features\", batch_size=32):\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mimage_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mimage_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/player-speed/object-detection/app/deep_sort/deep_sort/generate_detections.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, checkpoint_filename, input_name, output_name)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"net\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         self.input_var = tf.get_default_graph().get_tensor_by_name(\n\u001b[0;32m---> 81\u001b[0;31m             \"net/%s:0\" % input_name)\n\u001b[0m\u001b[1;32m     82\u001b[0m         self.output_var = tf.get_default_graph().get_tensor_by_name(\n\u001b[1;32m     83\u001b[0m             \"net/%s:0\" % output_name)\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_tensor_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3497\u001b[0m       raise TypeError(\"Tensor names are strings (or similar), not %s.\" %\n\u001b[1;32m   3498\u001b[0m                       type(name).__name__)\n\u001b[0;32m-> 3499\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3501\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_tensor_by_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3322\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3363\u001b[0m           raise KeyError(\"The name %s refers to a Tensor which does not \"\n\u001b[1;32m   3364\u001b[0m                          \u001b[0;34m\"exist. The operation, %s, does not exist in the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3365\u001b[0;31m                          \"graph.\" % (repr(name), repr(op_name)))\n\u001b[0m\u001b[1;32m   3366\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3367\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The name 'net/images:0' refers to a Tensor which does not exist. The operation, 'net/images', does not exist in the graph.\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from deep_sort.deep_sort import generate_detections\n",
    "from deep_sort.deep_sort import Detection\n",
    "\n",
    "DEEP_SORT_DIR = 'deep_sort'\n",
    "DETECTION_DIR = 'detections'\n",
    "\n",
    "VIDEO_DIR = 'videos'\n",
    "VIDEO_FILE = 'transition.mp4'\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(DEEP_SORT_DIR, 'checkpoint', 'mars-small128.ckpt-68577')\n",
    "\n",
    "encoder = generate_detections.create_box_encoder(CHECKPOINT_PATH)\n",
    "video = os.path.join(VIDEO_DIR, VIDEO_FILE)\n",
    "name_out = 'generated_detections.txt'\n",
    "detection_file = 'detections.txt'\n",
    "\n",
    "def generate_detections_features(encoder, video, name_out, detection_file):\n",
    "\n",
    "    \"\"\"Generate detections with features. Modification with video\"\"\"\n",
    "    detections_in = np.loadtxt(detection_file, delimiter=',')\n",
    "    detections_out = []\n",
    "\n",
    "    frame_indices = detections_in[:, 0].astype(np.int)\n",
    "    min_frame_idx = frame_indices.astype(np.int).min()\n",
    "    max_frame_idx = frame_indices.astype(np.int).max()\n",
    "\n",
    "    camera = cv2.VideoCapture(video)\n",
    "\n",
    "    for frame_idx in range(min_frame_idx, max_frame_idx + 1):\n",
    "        print(\"Frame %05d/%05d\" % (frame_idx, max_frame_idx))\n",
    "        mask = frame_indices == frame_idx\n",
    "        rows = detections_in[mask]\n",
    "\n",
    "        if frame_idx not in frame_indices:\n",
    "            print(\"WARNING could not find image for frame %d\" % frame_idx)\n",
    "            continue\n",
    "        camera.set(cv2.CAP_PROP_POS_FRAMES, frame_idx-1);\n",
    "        (grabbed, bgr_image) = camera.read()\n",
    "#         bgr_image = cv2.imread(image_filenames[frame_idx], cv2.IMREAD_COLOR)\n",
    "        features = encoder(bgr_image, rows[:, 2:6].copy())\n",
    "        detections_out += [np.r_[(row, feature)] for row, feature\n",
    "                           in zip(rows, features)]\n",
    "\n",
    "    # output_filename = os.path.join(output_dir, \"%s.npy\" % sequence)\n",
    "    np.save(name_out, np.asarray(detections_out), allow_pickle=False)\n",
    "    \n",
    "generate_detections_features(encoder, video, name_out, detection_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracking(detections_file):\n",
    "    \"\"\" Track objects\"\"\"\n",
    "    frame_idx = 0;\n",
    "    min_frame_idx = int(detections_file[:, 0].min())\n",
    "    max_frame_idx = int(detections_file[:, 0].max())\n",
    "    min_confidence = -1\n",
    "    min_detection_height = 0\n",
    "    nms_max_overlap = 0.3\n",
    "    \n",
    "    if (display):\n",
    "        plt.ion()\n",
    "        fig = plt.figure()\n",
    "\n",
    "    while True:\n",
    "        (grabbed, frame) = camera.read()\n",
    "        frame_idx += 1;\n",
    "        print(\"Processing frame %05d\" % frame_idx)\n",
    "\n",
    "        # Load image and generate detections.\n",
    "        detections = create_detections(detections_file, frame_idx-1, min_detection_height)\n",
    "        detections = [d for d in detections if d.confidence >= min_confidence]\n",
    "\n",
    "        # Run non-maxima suppression.\n",
    "        boxes = np.array([d.tlwh for d in detections])\n",
    "        scores = np.array([d.confidence for d in detections])\n",
    "        indices = preprocessing.non_max_suppression(boxes, nms_max_overlap, scores)\n",
    "        detections = [detections[i] for i in indices]\n",
    "\n",
    "        # Update tracker.\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "\n",
    "        # Update visualization.\n",
    "        if display:\n",
    "            ax1 = fig.add_subplot(111, aspect='equal')\n",
    "            # fn = 'mot_benchmark/%s/%s/img1/%06d.jpg'%(phase,seq,frame)\n",
    "            ax1.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(' Tracked Targets')\n",
    "\n",
    "        # Store results.\n",
    "        for track in tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            bbox = track.to_tlwh()\n",
    "            results.append([\n",
    "                frame_idx, track.track_id, bbox[0], bbox[1], bbox[2], bbox[3]])\n",
    "\n",
    "         \n",
    "            if (display):\n",
    "                ax1.add_patch(patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], fill=False, lw=3,ec=colours[track.track_id % 32, :]))\n",
    "                ax1.set_adjustable('box-forced')\n",
    "                plt.text(bbox[0], bbox[1], str(track.track_id))\n",
    "\n",
    "        if(display):\n",
    "            fig.canvas.flush_events()\n",
    "            plt.draw()\n",
    "            ax1.cla()\n",
    "\n",
    "        if grabbed == False:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
